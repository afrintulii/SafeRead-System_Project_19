{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1-ZwXEkIwc2"
      },
      "source": [
        "# Streamlit initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VkS7E6S_5Gi",
        "outputId": "14e445af-498e-4c40-a078-df90b3702914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unicode\n",
            "  Downloading unicode-2.9-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: unicode\n",
            "Successfully installed unicode-2.9\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.7\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "Successfully installed keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting xlrd==1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "Successfully installed xlrd-1.2.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Collecting bangla-stemmer\n",
            "  Downloading bangla_stemmer-1.0-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: bangla-stemmer\n",
            "Successfully installed bangla-stemmer-1.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (6,045 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120882 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
            "Fetched 186 kB in 1s (244 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120929 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-ben\n",
            "0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 516 kB of archives.\n",
            "After this operation, 870 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ben all 1:4.00~git30-7274cfa-1.1 [516 kB]\n",
            "Fetched 516 kB in 1s (983 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-ben.\n",
            "(Reading database ... 120959 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-ben_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.0.2.tar.gz (732 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.2/732.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-7.0.2-py3-none-any.whl size=21233 sha256=a28c5af6c76f26aaf4870215eca49cafa1d9ab89b1bff9704fe04821021f8dd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/3f/ca/3ee63fa3bf9dfcf6014bb5ea56026c8b218ad26b422fd9cce1\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.0.2\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install unicode\n",
        "! pip install unidecode\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow-gpu\n",
        "!pip install xlrd==1.2.0\n",
        "!pip install openpyxl\n",
        "!pip install bangla-stemmer\n",
        "!sudo apt install tesseract-ocr\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install pytesseract\n",
        "#install The pytesseract library using the command\n",
        "!sudo apt install tesseract-ocr-ben\n",
        "!pip install pdf2image\n",
        "!mkdir photos/\n",
        "!mkdir Text\n",
        "!touch input.txt\n",
        "!pip install pyngrok\n",
        "!pip install PyPDF2\n",
        "!pip install pdf2image\n",
        "!pip install python-docx\n",
        "!sudo apt-get install -y poppler-utils\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtKIFITnCpHq",
        "outputId": "e18b118b-a6a6-49aa-f1cc-ec6bc108fe7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,246 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,153 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,294 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,535 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,520 kB]\n",
            "Fetched 8,107 kB in 6s (1,470 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Update package list\n",
        "!apt-get update\n",
        "\n",
        "# Install poppler-utils\n",
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQiiAJa29reB",
        "outputId": "e11d4fa8-0c7d-4867-dcb5-6c005cbccef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y poppler-utils\n",
        "!sudo apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUDiIveq0em3",
        "outputId": "bae48463-3557-4e51-c430-d349d4d2d62c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "985IssNr0IMs"
      },
      "source": [
        "# Bnlp Toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9CbSVsR0C5S",
        "outputId": "52c2162b-c505-4d27-f869-a2ab5bf8f00a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bnlp_toolkit\n",
            "  Downloading bnlp_toolkit-4.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting sentencepiece (from bnlp_toolkit)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (4.3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (1.11.4)\n",
            "Collecting sklearn-crfsuite (from bnlp_toolkit)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (4.66.1)\n",
            "Collecting ftfy (from bnlp_toolkit)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji==1.7.0 (from bnlp_toolkit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (2.31.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->bnlp_toolkit) (0.2.12)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bnlp_toolkit) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (2023.11.17)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->bnlp_toolkit)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (0.9.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=8a9f86e8b42a60e24a31fe457ddde3775b0a6bbaa4f38379707d89a9b60b109a\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: sentencepiece, python-crfsuite, emoji, sklearn-crfsuite, ftfy, bnlp_toolkit\n",
            "Successfully installed bnlp_toolkit-4.0.0 emoji-1.7.0 ftfy-6.1.3 python-crfsuite-0.9.9 sentencepiece-0.1.99 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install bnlp_toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvMOn6ZierJj",
        "outputId": "efe3aede-9e02-44ab-f674-cac1eac8e994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'bnlp'...\n",
            "remote: Enumerating objects: 2048, done.\u001b[K\n",
            "remote: Counting objects: 100% (571/571), done.\u001b[K\n",
            "remote: Compressing objects: 100% (248/248), done.\u001b[K\n",
            "remote: Total 2048 (delta 338), reused 520 (delta 318), pack-reused 1477\u001b[K\n",
            "Receiving objects: 100% (2048/2048), 22.74 MiB | 25.23 MiB/s, done.\n",
            "Resolving deltas: 100% (1216/1216), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sagorbrur/bnlp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9xBKbhhe2B9",
        "outputId": "ce29d1ab-0f78-48bf-919b-478bd40a0e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/bnlp\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating bnlp_toolkit.egg-info\n",
            "writing bnlp_toolkit.egg-info/PKG-INFO\n",
            "writing dependency_links to bnlp_toolkit.egg-info/dependency_links.txt\n",
            "writing requirements to bnlp_toolkit.egg-info/requires.txt\n",
            "writing top-level names to bnlp_toolkit.egg-info/top_level.txt\n",
            "writing manifest file 'bnlp_toolkit.egg-info/SOURCES.txt'\n",
            "reading manifest file 'bnlp_toolkit.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'bnlp_toolkit.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/bnlp\n",
            "copying bnlp/__init__.py -> build/lib/bnlp\n",
            "creating build/lib/bnlp/corpus\n",
            "copying bnlp/corpus/__init__.py -> build/lib/bnlp/corpus\n",
            "copying bnlp/corpus/_stopwords.py -> build/lib/bnlp/corpus\n",
            "copying bnlp/corpus/corpus.py -> build/lib/bnlp/corpus\n",
            "creating build/lib/bnlp/tokenizer\n",
            "copying bnlp/tokenizer/nltk.py -> build/lib/bnlp/tokenizer\n",
            "copying bnlp/tokenizer/__init__.py -> build/lib/bnlp/tokenizer\n",
            "copying bnlp/tokenizer/sentencepiece.py -> build/lib/bnlp/tokenizer\n",
            "copying bnlp/tokenizer/basic.py -> build/lib/bnlp/tokenizer\n",
            "creating build/lib/bnlp/utils\n",
            "copying bnlp/utils/utils.py -> build/lib/bnlp/utils\n",
            "copying bnlp/utils/__init__.py -> build/lib/bnlp/utils\n",
            "copying bnlp/utils/config.py -> build/lib/bnlp/utils\n",
            "copying bnlp/utils/downloader.py -> build/lib/bnlp/utils\n",
            "creating build/lib/bnlp/embedding\n",
            "copying bnlp/embedding/word2vec.py -> build/lib/bnlp/embedding\n",
            "copying bnlp/embedding/fasttext.py -> build/lib/bnlp/embedding\n",
            "copying bnlp/embedding/__init__.py -> build/lib/bnlp/embedding\n",
            "copying bnlp/embedding/doc2vec.py -> build/lib/bnlp/embedding\n",
            "copying bnlp/embedding/glove.py -> build/lib/bnlp/embedding\n",
            "creating build/lib/bnlp/token_classification\n",
            "copying bnlp/token_classification/ner.py -> build/lib/bnlp/token_classification\n",
            "copying bnlp/token_classification/token_classification_trainer.py -> build/lib/bnlp/token_classification\n",
            "copying bnlp/token_classification/__init__.py -> build/lib/bnlp/token_classification\n",
            "copying bnlp/token_classification/pos.py -> build/lib/bnlp/token_classification\n",
            "creating build/lib/bnlp/cleantext\n",
            "copying bnlp/cleantext/__init__.py -> build/lib/bnlp/cleantext\n",
            "copying bnlp/cleantext/constants.py -> build/lib/bnlp/cleantext\n",
            "copying bnlp/cleantext/clean.py -> build/lib/bnlp/cleantext\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/bnlp\n",
            "creating build/bdist.linux-x86_64/egg/bnlp/corpus\n",
            "copying build/lib/bnlp/corpus/__init__.py -> build/bdist.linux-x86_64/egg/bnlp/corpus\n",
            "copying build/lib/bnlp/corpus/_stopwords.py -> build/bdist.linux-x86_64/egg/bnlp/corpus\n",
            "copying build/lib/bnlp/corpus/corpus.py -> build/bdist.linux-x86_64/egg/bnlp/corpus\n",
            "creating build/bdist.linux-x86_64/egg/bnlp/tokenizer\n",
            "copying build/lib/bnlp/tokenizer/nltk.py -> build/bdist.linux-x86_64/egg/bnlp/tokenizer\n",
            "copying build/lib/bnlp/tokenizer/__init__.py -> build/bdist.linux-x86_64/egg/bnlp/tokenizer\n",
            "copying build/lib/bnlp/tokenizer/sentencepiece.py -> build/bdist.linux-x86_64/egg/bnlp/tokenizer\n",
            "copying build/lib/bnlp/tokenizer/basic.py -> build/bdist.linux-x86_64/egg/bnlp/tokenizer\n",
            "copying build/lib/bnlp/__init__.py -> build/bdist.linux-x86_64/egg/bnlp\n",
            "creating build/bdist.linux-x86_64/egg/bnlp/utils\n",
            "copying build/lib/bnlp/utils/utils.py -> build/bdist.linux-x86_64/egg/bnlp/utils\n",
            "copying build/lib/bnlp/utils/__init__.py -> build/bdist.linux-x86_64/egg/bnlp/utils\n",
            "copying build/lib/bnlp/utils/config.py -> build/bdist.linux-x86_64/egg/bnlp/utils\n",
            "copying build/lib/bnlp/utils/downloader.py -> build/bdist.linux-x86_64/egg/bnlp/utils\n",
            "creating build/bdist.linux-x86_64/egg/bnlp/embedding\n",
            "copying build/lib/bnlp/embedding/word2vec.py -> build/bdist.linux-x86_64/egg/bnlp/embedding\n",
            "copying build/lib/bnlp/embedding/fasttext.py -> build/bdist.linux-x86_64/egg/bnlp/embedding\n",
            "copying build/lib/bnlp/embedding/__init__.py -> build/bdist.linux-x86_64/egg/bnlp/embedding\n",
            "copying build/lib/bnlp/embedding/doc2vec.py -> build/bdist.linux-x86_64/egg/bnlp/embedding\n",
            "copying build/lib/bnlp/embedding/glove.py -> build/bdist.linux-x86_64/egg/bnlp/embedding\n",
            "creating build/bdist.linux-x86_64/egg/bnlp/token_classification\n",
            "copying build/lib/bnlp/token_classification/ner.py -> build/bdist.linux-x86_64/egg/bnlp/token_classification\n",
            "copying build/lib/bnlp/token_classification/token_classification_trainer.py -> build/bdist.linux-x86_64/egg/bnlp/token_classification\n",
            "copying build/lib/bnlp/token_classification/__init__.py -> build/bdist.linux-x86_64/egg/bnlp/token_classification\n",
            "copying build/lib/bnlp/token_classification/pos.py -> build/bdist.linux-x86_64/egg/bnlp/token_classification\n",
            "creating build/bdist.linux-x86_64/egg/bnlp/cleantext\n",
            "copying build/lib/bnlp/cleantext/__init__.py -> build/bdist.linux-x86_64/egg/bnlp/cleantext\n",
            "copying build/lib/bnlp/cleantext/constants.py -> build/bdist.linux-x86_64/egg/bnlp/cleantext\n",
            "copying build/lib/bnlp/cleantext/clean.py -> build/bdist.linux-x86_64/egg/bnlp/cleantext\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/corpus/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/corpus/_stopwords.py to _stopwords.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/corpus/corpus.py to corpus.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/tokenizer/nltk.py to nltk.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/tokenizer/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/tokenizer/sentencepiece.py to sentencepiece.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/tokenizer/basic.py to basic.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/utils/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/utils/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/utils/config.py to config.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/utils/downloader.py to downloader.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/embedding/word2vec.py to word2vec.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/embedding/fasttext.py to fasttext.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/embedding/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/embedding/doc2vec.py to doc2vec.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/embedding/glove.py to glove.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/token_classification/ner.py to ner.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/token_classification/token_classification_trainer.py to token_classification_trainer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/token_classification/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/token_classification/pos.py to pos.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/cleantext/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/cleantext/constants.py to constants.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bnlp/cleantext/clean.py to clean.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bnlp_toolkit.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bnlp_toolkit.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bnlp_toolkit.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bnlp_toolkit.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bnlp_toolkit.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/bnlp_toolkit-4.0.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing bnlp_toolkit-4.0.0-py3.10.egg\n",
            "Copying bnlp_toolkit-4.0.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding bnlp-toolkit 4.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/bnlp_toolkit-4.0.0-py3.10.egg\n",
            "Processing dependencies for bnlp-toolkit==4.0.0\n",
            "Searching for requests==2.31.0\n",
            "Best match: requests 2.31.0\n",
            "Adding requests 2.31.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for emoji==1.7.0\n",
            "Best match: emoji 1.7.0\n",
            "Adding emoji 1.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for ftfy==6.1.3\n",
            "Best match: ftfy 6.1.3\n",
            "Adding ftfy 6.1.3 to easy-install.pth file\n",
            "Installing ftfy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tqdm==4.66.1\n",
            "Best match: tqdm 4.66.1\n",
            "Adding tqdm 4.66.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for sklearn-crfsuite==0.3.6\n",
            "Best match: sklearn-crfsuite 0.3.6\n",
            "Adding sklearn-crfsuite 0.3.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for scipy==1.11.4\n",
            "Best match: scipy 1.11.4\n",
            "Adding scipy 1.11.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numpy==1.23.5\n",
            "Best match: numpy 1.23.5\n",
            "Adding numpy 1.23.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.10 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for nltk==3.8.1\n",
            "Best match: nltk 3.8.1\n",
            "Adding nltk 3.8.1 to easy-install.pth file\n",
            "Installing nltk script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for gensim==4.3.2\n",
            "Best match: gensim 4.3.2\n",
            "Adding gensim 4.3.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for sentencepiece==0.1.99\n",
            "Best match: sentencepiece 0.1.99\n",
            "Adding sentencepiece 0.1.99 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for certifi==2023.11.17\n",
            "Best match: certifi 2023.11.17\n",
            "Adding certifi 2023.11.17 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for urllib3==2.0.7\n",
            "Best match: urllib3 2.0.7\n",
            "Adding urllib3 2.0.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for idna==3.6\n",
            "Best match: idna 3.6\n",
            "Adding idna 3.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for charset-normalizer==3.3.2\n",
            "Best match: charset-normalizer 3.3.2\n",
            "Adding charset-normalizer 3.3.2 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for wcwidth==0.2.12\n",
            "Best match: wcwidth 0.2.12\n",
            "Adding wcwidth 0.2.12 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tabulate==0.9.0\n",
            "Best match: tabulate 0.9.0\n",
            "Adding tabulate 0.9.0 to easy-install.pth file\n",
            "Installing tabulate script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for six==1.16.0\n",
            "Best match: six 1.16.0\n",
            "Adding six 1.16.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for python-crfsuite==0.9.9\n",
            "Best match: python-crfsuite 0.9.9\n",
            "Adding python-crfsuite 0.9.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for regex==2023.6.3\n",
            "Best match: regex 2023.6.3\n",
            "Adding regex 2023.6.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for joblib==1.3.2\n",
            "Best match: joblib 1.3.2\n",
            "Adding joblib 1.3.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for click==8.1.7\n",
            "Best match: click 8.1.7\n",
            "Adding click 8.1.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for smart-open==6.4.0\n",
            "Best match: smart-open 6.4.0\n",
            "Adding smart-open 6.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for bnlp-toolkit==4.0.0\n"
          ]
        }
      ],
      "source": [
        "%cd bnlp\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sQWSvvZe9Fh"
      },
      "outputs": [],
      "source": [
        "from bnlp import NLTKTokenizer\n",
        "from bnlp import CleanText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhRNHs8Ve99_",
        "outputId": "83339da4-de85-4c87-a88b-d5bb217d8f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DwJMVTmIYmsr",
        "outputId": "382fb876-eb73-4251-f422-b3b6d492aeda"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoAQgjrbBY3Q",
        "outputId": "4d2f8b66-62eb-499b-9a07-a32ab8a81858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi) (1.16.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install python-bidi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74u8D-VsufEY",
        "outputId": "96598893-3d32-43f9-dd62-05b7c979b0de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from wordcloud import WordCloud\n",
        "from matplotlib import pyplot as plt\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "#import plost\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import os\n",
        "import io\n",
        "from pandas import read_excel\n",
        "import re\n",
        "from re import sub\n",
        "import multiprocessing\n",
        "from unidecode import unidecode\n",
        "from time import time\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation, Embedding, Flatten, Bidirectional, MaxPooling2D, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "import h5py\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import pytesseract\n",
        "import shutil\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pytesseract\n",
        "\n",
        "\n",
        "import docx\n",
        "\n",
        "\n",
        "import pytesseract\n",
        "from PIL import ImageEnhance, ImageFilter, Image\n",
        "\n",
        "# #installing poppler\n",
        "# !apt-get install -y poppler-utils\n",
        "#import module\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from PyPDF2 import PdfFileReader\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "import streamlit as st\n",
        "\n",
        "from PyPDF2 import PdfReader  # Change this line\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "from PIL import Image\n",
        "import io\n",
        "import concurrent.futures\n",
        "\n",
        "\n",
        "from bangla_stemmer.stemmer import stemmer\n",
        "\n",
        "from bnlp import NLTKTokenizer\n",
        "from bnlp import CleanText\n",
        "\n",
        "#dataset import\n",
        "\n",
        "#Cutom Website Design\n",
        "st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "st.set_page_config(layout='wide', initial_sidebar_state='expanded')\n",
        "with open('/content/drive/MyDrive/Custom Website/style.css') as f:\n",
        "    st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)\n",
        "st.sidebar.header('SafeRead')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load CSV files\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/Religious Dictionary.csv\")\n",
        "df_political = pd.read_csv(\"/content/drive/MyDrive/DATASETS/Political Dictionary .csv\")\n",
        "df_abusive = pd.read_csv(\"/content/drive/MyDrive/DATASETS/Abuse Dictionary.csv\")\n",
        "\n",
        "\n",
        "#\n",
        "def text_to_word_list(text):\n",
        "    text = text.split()\n",
        "    return text\n",
        "\n",
        "def replace_strings(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           u\"\\u00C0-\\u017F\"          #latin\n",
        "                           u\"\\u2000-\\u206F\"          #generalPunctuations\n",
        "\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
        "    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n",
        "\n",
        "    text=emoji_pattern.sub(r'', text)\n",
        "    text=english_pattern.sub(r'', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_punctuations(my_str):\n",
        "    # define punctuation\n",
        "    punctuations = '''```\u0012\u0010\u0002\b`\u0007\b£|¢|\u0007Ñ+-*/=EROero৳০১২৩৪৫৬৭৮৯012–34567•89।!()-[]{};:'\"“\\’,<>./?@#$%^&*_~‘—॥”‰🤣⚽️✌�￰৷￰'''\n",
        "\n",
        "    no_punct = \"\"\n",
        "    for char in my_str:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "\n",
        "    # display the unpunctuated string\n",
        "    return no_punct\n",
        "\n",
        "\n",
        "\n",
        "def joining(text):\n",
        "    out=' '.join(text)\n",
        "    return out\n",
        "\n",
        "def preprocessing(text):\n",
        "    out=remove_punctuations(replace_strings(text))\n",
        "    return out\n",
        "#stop word removal\n",
        "def stopwordRemoval(text):\n",
        "    x=str(text)\n",
        "    l=x.split()\n",
        "\n",
        "    stm=[elem for elem in l if elem not in stop]\n",
        "\n",
        "    out=' '.join(stm)\n",
        "\n",
        "    return str(out)\n",
        "\n",
        "def stem_text (x):\n",
        "  stmr = stemmer.BanglaStemmer()\n",
        "  words=x.split(' ')\n",
        "  stm = stmr.stem(words)\n",
        "  words=(' ').join(stm)\n",
        "  return words\n",
        "\n",
        "def generate_N_grams(text,ngram):\n",
        "  words=[word for word in text.split(\" \")]\n",
        " # print(\"Sentence after removing stopwords:\",words)\n",
        "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
        "  ans=[' '.join(ngram) for ngram in temp]\n",
        "  return ans\n",
        "\n",
        "\n",
        "def create_pie_chart(data):\n",
        "    import plotly.express as px\n",
        "    fig = px.pie(data, names='Category', values='Percentage', title='Word Category Percentages')\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data1 =pd.read_excel('/content/drive/MyDrive/DATASETS/stopwords_bangla.xlsx')\n",
        "stop = data1['words'].tolist()\n",
        "\n",
        "\n",
        "# Create a function to get the number of pages in a PDF\n",
        "def get_pdf_page_count(pdf_path):\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PdfReader(pdf_file)\n",
        "        return len(pdf_reader.pages)\n",
        "\n",
        "# Create a function to convert PDF to images\n",
        "def convert_pdf_to_images(pdf_path):\n",
        "    # Convert PDF to a list of images\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    # Save each page as a PNG in /content/photos directory\n",
        "    output_folder = '/content/photos'\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        image.save(f'{output_folder}/page_{i + 1}.png', 'PNG')\n",
        "\n",
        "# Create a function to process pages and return a DataFrame\n",
        "def process_page(page_number, image):\n",
        "    try:\n",
        "        # Convert PIL Image to bytes\n",
        "        img_byte_array = io.BytesIO()\n",
        "        image.save(img_byte_array, format='PNG')\n",
        "        img_byte_array = img_byte_array.getvalue()\n",
        "\n",
        "        # Use Tesseract OCR on the image\n",
        "        extracted_text = pytesseract.image_to_string(Image.open(io.BytesIO(img_byte_array)), lang='eng+ben')\n",
        "\n",
        "        return {'Page Number': page_number, 'Extracted Text': extracted_text}\n",
        "    except Exception as e:\n",
        "        return {'Page Number': page_number, 'Extracted Text': f'Error: {str(e)}'}\n",
        "\n",
        "# Create a function to process pages and return a DataFrame\n",
        "def process_pdf_to_dataframe(pdf_path):\n",
        "    import pytesseract\n",
        "    import pandas as pd\n",
        "    from pdf2image import convert_from_path\n",
        "    from PIL import Image\n",
        "    import io\n",
        "    import concurrent.futures\n",
        "    images = convert_from_path(pdf_path)\n",
        "    df_test = pd.DataFrame(columns=['Page Number', 'Extracted Text'])\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        results = [executor.submit(process_page, i + 1, image) for i, image in enumerate(images)]\n",
        "\n",
        "        # Collect results\n",
        "        for future in concurrent.futures.as_completed(results):\n",
        "            df_test = df_test.append(future.result(), ignore_index=True)\n",
        "\n",
        "    return df_test\n",
        "\n",
        "from bnlp import NLTKTokenizer\n",
        "from bnlp import CleanText\n",
        "# Create an instance of the tokenizer\n",
        "bnltk = NLTKTokenizer()\n",
        "# Define a function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return bnltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "\n",
        "#INPUT TEXT FILE TO DATAFRAME\n",
        "def convert_text_to_dataframe(text_file):\n",
        "    df_text2 = pd.read_csv(text_file, names=['Extracted Text'], header=None, delimiter='\\t')\n",
        "    return df_text2\n",
        "\n",
        "\n",
        "\n",
        "# Input Text to Dataframe conversion\n",
        "def text_to_dataframe(bangla_text):\n",
        "    # Split the Bangla text into rows (assuming each line is a separate row)\n",
        "    rows = bangla_text.split('\\n')\n",
        "\n",
        "    # Create a DataFrame from the rows\n",
        "    df_text = pd.DataFrame(rows, columns=['Extracted Text'])\n",
        "\n",
        "    return df_text\n",
        "\n",
        "\n",
        "def convert_word_to_dataframe(upload_file):\n",
        "    # Load the Word document\n",
        "    doc = docx.Document(upload_file)\n",
        "\n",
        "    # Initialize an empty list to store paragraphs\n",
        "    paragraphs = []\n",
        "\n",
        "    # Iterate through paragraphs in the document and extract text\n",
        "    for paragraph in doc.paragraphs:\n",
        "        paragraphs.append(paragraph.text)\n",
        "\n",
        "    # Create a DataFrame from the list of paragraphs\n",
        "    df_text = pd.DataFrame({'Extracted Text': paragraphs})\n",
        "\n",
        "    return df_text\n",
        "\n",
        "\n",
        "def analyse(df_test):\n",
        "            #df_test = process_pdf_to_dataframe(pdf_path)\n",
        "            #st.dataframe(df_test)\n",
        "            #st.dataframe(df_political)\n",
        "            df['cleanText'] = df['Keyword'].apply(lambda x: preprocessing(str(x)))\n",
        "            df_political['cleanText'] = df_political['Keyword'].apply(lambda x: preprocessing(str(x)))\n",
        "            df_abusive['cleanText'] = df_abusive['Keyword'].apply(lambda x: preprocessing(str(x)))\n",
        "            df_test['Extracted Text'] = df_test['Extracted Text'].apply(lambda x: preprocessing(str(x)))\n",
        "            from bnlp import NLTKTokenizer\n",
        "            from bnlp import CleanText\n",
        "            df['cleanText']=df['cleanText'].apply(stem_text)\n",
        "            df_political['cleanText']=df_political['cleanText'].apply(stem_text)\n",
        "            df_abusive['cleanText']=df_abusive['cleanText'].apply(stem_text)\n",
        "            df_test['cleanText']=df_test['Extracted Text'].apply(stem_text)\n",
        "            #stopword removal\n",
        "            df['cleanText'] = df['cleanText'].apply(lambda x: stopwordRemoval(str(x)))\n",
        "            df_political['cleanText'] = df_political['cleanText'].apply(lambda x: stopwordRemoval(str(x)))\n",
        "            df_abusive['cleanText'] = df_abusive['cleanText'].apply(lambda x: stopwordRemoval(str(x)))\n",
        "            df_test['Extracted Text'] = df_test['Extracted Text'].apply(lambda x: stopwordRemoval(str(x)))\n",
        "            #st.write(\"stemming done.....\")\n",
        "            # Create an instance of the tokenizer\n",
        "            bnltk = NLTKTokenizer()\n",
        "            # Define a function to tokenize text\n",
        "            def tokenize_text(text):\n",
        "                return bnltk.word_tokenize(text)\n",
        "            df_test['Tokenized Text'] = df_test['cleanText'].apply(tokenize_text)\n",
        "            #st.write(\"tokenizing done.....\")\n",
        "            #st.dataframe(df_test)\n",
        "            y=df_test['Extracted Text'].values\n",
        "\n",
        "\n",
        "\n",
        "            unigram=df_test['cleanText'].apply(lambda x: generate_N_grams(x, ngram=1))\n",
        "\n",
        "            unigram_test=pd.DataFrame(unigram)\n",
        "            # Explode the tokenized text into individual rows\n",
        "            df_test_exploded = unigram_test['cleanText'].explode().reset_index(drop=True)\n",
        "\n",
        "            # Create a new DataFrame with the 'keyword' column\n",
        "            df_test2 = pd.DataFrame({'keyword': df_test_exploded})\n",
        "            # Display the resulting DataFrame\n",
        "            total_word_in_input= len(df_test2)\n",
        "            df_test2.value_counts()\n",
        "            word_counts = df_test2.value_counts()\n",
        "            search_keys = list(df[\"Keyword\"].dropna())\n",
        "            search_keys_political=list(df_political[\"Keyword\"].dropna())\n",
        "            search_keys_abusive=list(df_abusive[\"Keyword\"].dropna())\n",
        "\n",
        "\n",
        "\n",
        "            total_count_religious=0\n",
        "            total_count_political=0\n",
        "            total_count_abusive=0\n",
        "            found_keys_religious = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keys:\n",
        "                found_keys_religious[word[0]] = count\n",
        "\n",
        "\n",
        "            found_keys_political = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keys_political:\n",
        "                found_keys_political[word[0]] = count\n",
        "                #print(found_keys_political[word[0]] = count)\n",
        "\n",
        "            found_keys_abusive = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keys_abusive:\n",
        "                found_keys_abusive[word[0]] = count\n",
        "            total_count_religious = total_count_religious+sum(found_keys_religious.values())\n",
        "            total_count_political = total_count_political+sum(found_keys_political.values())\n",
        "            total_count_abusive= total_count_abusive+sum(found_keys_abusive.values())\n",
        "            #st.write(\"abu uni\",total_count_abusive)\n",
        "            bigram=df_test['cleanText'].apply(lambda x: generate_N_grams(x, ngram=2))\n",
        "            bigram_test=pd.DataFrame(bigram)\n",
        "\n",
        "            # Explode the tokenized text into individual rows\n",
        "            df_test_explodedB = bigram_test['cleanText'].explode().reset_index(drop=True)\n",
        "\n",
        "            # Create a new DataFrame with the 'keyword' column\n",
        "            df_test2B = pd.DataFrame({'keyword': df_test_explodedB})\n",
        "            df_test2B.value_counts()\n",
        "\n",
        "            word_counts = df_test2B.value_counts()\n",
        "            search_keysB = list(df[\"Keyword\"].dropna())\n",
        "            search_keysB_political=list(df_political[\"Keyword\"].dropna())\n",
        "            search_keysB_abusive=list(df_abusive[\"Keyword\"].dropna())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            found_keysB_religious = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keysB:\n",
        "                found_keysB_religious[word[0]] = count\n",
        "\n",
        "\n",
        "            found_keysB_political = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keysB_political:\n",
        "                found_keysB_political[word[0]] = count\n",
        "\n",
        "            found_keysB_abusive = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keysB_abusive:\n",
        "                found_keysB_abusive[word[0]] = count\n",
        "\n",
        "            total_count_religious = total_count_religious+sum(found_keysB_religious.values())\n",
        "            total_count_political = total_count_political+sum(found_keys_political.values())\n",
        "            #total_count_abusive= total_count_abusive+sum(found_keysB_abusive.values())\n",
        "            suma=(sum(found_keysB_religious.values())+sum(found_keysB_political.values())+sum(found_keysB_abusive.values()))\n",
        "            total_word_in_input = total_word_in_input - suma *3 + suma\n",
        "\n",
        "            #st.write(\"bi ab\",total_count_abusive)\n",
        "            trigram=df_test['cleanText'].apply(lambda x: generate_N_grams(x, ngram=3))\n",
        "            trigram_test=pd.DataFrame(trigram)\n",
        "            # Explode the tokenized text into individual rows\n",
        "            df_test_explodedT = trigram_test['cleanText'].explode().reset_index(drop=True)\n",
        "\n",
        "            # Create a new DataFrame with the 'keyword' column\n",
        "            df_test2T = pd.DataFrame({'keyword': df_test_explodedT})\n",
        "\n",
        "            # Display the resulting DataFrame\n",
        "\n",
        "            df_test2T.value_counts()\n",
        "            word_counts = df_test2T.value_counts()\n",
        "            search_keysT = list(df[\"Keyword\"].dropna())\n",
        "            search_keysT_political=list(df_political[\"Keyword\"].dropna())\n",
        "            search_keysT_abusive=list(df_abusive[\"Keyword\"].dropna())\n",
        "\n",
        "            #for Word CLOUD\n",
        "\n",
        "            political_words_unigram = df_test2[df_test2['keyword'].isin(search_keys_political)]['keyword'].tolist()\n",
        "            religious_words_unigram = df_test2[df_test2['keyword'].isin(search_keys)]['keyword'].tolist()\n",
        "            abusive_words_unigram = df_test2[df_test2['keyword'].isin(search_keys_abusive)]['keyword'].tolist()\n",
        "\n",
        "            political_words_bigram = df_test2B[df_test2B['keyword'].isin(search_keysB_political)]['keyword'].tolist()\n",
        "            religious_words_bigram = df_test2B[df_test2B['keyword'].isin(search_keysB)]['keyword'].tolist()\n",
        "            abusive_words_bigram = df_test2B[df_test2B['keyword'].isin(search_keysB_abusive)]['keyword'].tolist()\n",
        "\n",
        "            political_words_trigram = df_test2T[df_test2T['keyword'].isin(search_keysT_political)]['keyword'].tolist()\n",
        "            religious_words_trigram = df_test2T[df_test2T['keyword'].isin(search_keysT)]['keyword'].tolist()\n",
        "            abusive_words_trigram = df_test2T[df_test2T['keyword'].isin(search_keysT_abusive)]['keyword'].tolist()\n",
        "\n",
        "            # Combine all lists\n",
        "            religious_words = religious_words_unigram + religious_words_bigram + religious_words_trigram\n",
        "\n",
        "            political_words = political_words_unigram  + political_words_bigram + political_words_trigram\n",
        "\n",
        "            abusive_words = abusive_words_unigram + abusive_words_bigram + abusive_words_trigram\n",
        "\n",
        "            # Convert the list to a string\n",
        "            text_religious = ' '.join(religious_words)\n",
        "            text_political = ' '.join(political_words)\n",
        "            text_abusive = ' '.join(abusive_words)\n",
        "\n",
        "            #st.write(\"Text Religious\")\n",
        "            #st.write(text_religious)\n",
        "            #st.write(\"Text Plotical\")\n",
        "            #st.write(text_political)\n",
        "            #st.write(\"Text Abusive\")\n",
        "            #st.write(text_abusive)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            found_keysT_religious = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keysT:\n",
        "                found_keysT_religious[word[0]] = count\n",
        "\n",
        "\n",
        "            found_keysT_political = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keysT_political:\n",
        "                found_keysT_political[word[0]] = count\n",
        "\n",
        "            found_keysT_abusive = {}\n",
        "            for word, count in zip(word_counts.keys(), word_counts.values):\n",
        "              if word[0] in search_keysT_abusive:\n",
        "                found_keysT_abusive[word[0]] = count\n",
        "            total_count_religious = total_count_religious+sum(found_keysT_religious.values())\n",
        "            total_count_political = total_count_political+sum(found_keysT_political.values())\n",
        "            #total_count_abusive= total_count_abusive+sum(found_keysT_abusive.values())\n",
        "            sumt=(sum(found_keysB_religious.values())+sum(found_keysT_political.values())+sum(found_keysT_abusive.values()))\n",
        "            total_word_in_input = total_word_in_input - sumt *3 + sumt\n",
        "            religious_key = total_count_religious / total_word_in_input\n",
        "            political_key = total_count_political / total_word_in_input\n",
        "            abusive_key = total_count_abusive / total_word_in_input\n",
        "            st.title(\"Bangla Content Classifier and Recommendation System\")\n",
        "            st.write(\"Religious Words :\",total_count_religious)\n",
        "            st.write(\"Political Words :\",total_count_political)\n",
        "            st.write(\"Abusive Words :\",total_count_abusive)\n",
        "\n",
        "            # Display the calculated percentages\n",
        "            st.write(\"Word Percentages:\")\n",
        "            st.write(f\"Religious words: {round(religious_key * 100, 2)}%\")\n",
        "\n",
        "            st.write(f\"Political words: {round(political_key * 100, 2)}%\")\n",
        "            st.write(f\"Abusive words: {round(abusive_key * 100, 2)}%\")\n",
        "\n",
        "            # Create a pie chart to visualize the percentages\n",
        "            chart_data = pd.DataFrame({\n",
        "                        'Category': ['Religious', 'Political', 'Abusive'],\n",
        "                        'Percentage': [religious_key, political_key, abusive_key]\n",
        "            })\n",
        "\n",
        "\n",
        "            #Print the Word Cloud\n",
        "            st.write(\"Word CLOUD for Matched Religious Keyword: \")\n",
        "            if len(text_religious) != 0:\n",
        "                bidi_text = get_display(text_religious)\n",
        "                rgx = r\"[\\u0980-\\u09FF]+\"\n",
        "                wordcloud = WordCloud(font_path='/content/drive/MyDrive/DATASETS/Siyamrupali.ttf',background_color='white', regexp=rgx).generate(bidi_text)\n",
        "                plt.imshow(wordcloud, interpolation='bilinear')\n",
        "                plt.axis(\"off\")\n",
        "                st.pyplot()\n",
        "\n",
        "            else:\n",
        "                st.write(\"No Religious word is present\")\n",
        "            #\n",
        "\n",
        "\n",
        "\n",
        "            st.write(\"Word CLOUD for Matched Political Keyword: \")\n",
        "            if len(text_political) != 0:\n",
        "                bidi_text = get_display(text_political)\n",
        "                rgx = r\"[\\u0980-\\u09FF]+\"\n",
        "                w = WordCloud(font_path='/content/drive/MyDrive/DATASETS/Siyamrupali.ttf',background_color='white', regexp=rgx).generate(bidi_text)\n",
        "                plt.imshow(w, interpolation='bilinear')\n",
        "                plt.axis(\"off\")\n",
        "                st.pyplot()\n",
        "\n",
        "            else:\n",
        "                st.write(\"No Political word is present\")\n",
        "            #\n",
        "            st.write(\"Word CLOUD for Matched Abusive Keyword: \")\n",
        "            if len(text_abusive) != 0:\n",
        "                bidi_text = get_display(text_abusive)\n",
        "                rgx = r\"[\\u0980-\\u09FF]+\"\n",
        "                w_abusive = WordCloud(font_path='/content/drive/MyDrive/DATASETS/Siyamrupali.ttf',background_color='white', regexp=rgx).generate(bidi_text)\n",
        "                plt.imshow(w_abusive, interpolation='bilinear')\n",
        "                plt.axis(\"off\")\n",
        "                st.pyplot()\n",
        "            else:\n",
        "\n",
        "                 st.write(\"No Abusive word is present\")\n",
        "\n",
        "            #FiNISHED\n",
        "            st.write(\"### Pie Chart:\")\n",
        "            st.plotly_chart(create_pie_chart(chart_data), use_container_width=True)\n",
        "\n",
        "            # Decision-making logic\n",
        "            categories = {'Religious': religious_key, 'Political': political_key, 'Abusive': abusive_key}\n",
        "            predicted_category = max(categories, key=categories.get)\n",
        "            st.write(f\"### Predicted Category: {predicted_category}\")\n",
        "            if abusive_key <= 0.003 and political_key <= 0.03:\n",
        "                st.write(\"This book is suitable for 8 years to 10 years children\")\n",
        "            elif abusive_key <= 0.003 and political_key <= 0.09:\n",
        "                st.write(\"This book is suitable for 10 to 15 years children\")\n",
        "            elif abusive_key >= 0.01:\n",
        "                st.write('<span style=\"color:red;\">It is an adult content. This book is not suitable for underaged individuals</span>', unsafe_allow_html=True)\n",
        "            else:\n",
        "                st.write(\"This book is suitable for 15 years to 18 years children\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_file_extension(file_name):\n",
        "    # Split the file name and get the last part (after the last dot)\n",
        "    parts = file_name.split(\".\")\n",
        "    #st.write(parts[-1])\n",
        "    if len(parts) > 1:\n",
        "        return parts[-1]\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    #Upload a  file\n",
        "    upload_file = st.sidebar.file_uploader(\"Choose a File\", type=[\"pdf\", \"txt\", \"docx\"])\n",
        "    if upload_file is not None:\n",
        "        st.sidebar.text(\"File successfully uploaded!\")\n",
        "        file_extension = get_file_extension(upload_file.name)\n",
        "        if file_extension == \"txt\":\n",
        "            df_test = convert_text_to_dataframe(upload_file)\n",
        "        elif file_extension == \"docx\":\n",
        "            df_test= convert_word_to_dataframe(upload_file)\n",
        "\n",
        "        elif file_extension == \"pdf\":\n",
        "            # Save uploaded PDF file to '/content/' path\n",
        "            pdf_path = \"/content/uploaded_file.pdf\"\n",
        "            with open(pdf_path, \"wb\") as f:\n",
        "                f.write(upload_file.getvalue())\n",
        "                df_test = process_pdf_to_dataframe(pdf_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if st.sidebar.button(\"Show Analysis\"):\n",
        "            analyse(df_test)\n",
        "\n",
        "  # User Input text\n",
        "    user_input = st.sidebar.text_area(\"Enter text:\")\n",
        "    if st.sidebar.button(\"Show Prediction\"):\n",
        "        df_test = text_to_dataframe(user_input)\n",
        "        #st.sidebar.write(\"Complete\")\n",
        "        #st.dataframe(df_test)\n",
        "        analyse(df_test)\n",
        "\n",
        "\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA3E-KryNCqE",
        "outputId": "e5010746-53b3-431d-93cb-812d6c6b29a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.173.200.251\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.34s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.924s\n",
            "your url is: https://nasty-mugs-own.loca.lt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!npm install localtunnel\n",
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501\n",
        "#34.173.200.251\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErUgVe8gSOq9"
      },
      "source": [
        "সরকার ও গণতন্ত্রের মাধ্যমে দেশের নীতি নির্ধারণ করা হোক এবং জনগণের মৌলিক অধিকার এবং প্রজাতন্ত্র নীতি সমর্থন করা হোক।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYp5e8O9SOnf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS3n-IoHD6dX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}